{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82ceb9b6-4046-4128-bcf5-ec8313cd515b",
   "metadata": {},
   "source": [
    "# Ans : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cee684-98b6-40d1-b295-c1e161a7d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Simple linear regression involves predicting the values of one dependent variable based on the values of a single independent \n",
    "variable. The relationship is represented by a straight line equation, where the dependent variable is a linear function of\n",
    "the independent variable. For example, predicting a student's exam score (dependent variable) based on the number of hours \n",
    "spent studying (independent variable) is a simple linear regression.\n",
    "\n",
    "On the other hand, multiple linear regression extends this concept to include two or more independent variables to predict the\n",
    "dependent variable. The relationship is represented by a plane or hyperplane in higher dimensions. For instance, predicting a\n",
    "person's salary (dependent variable) based on both years of experience and education level (two independent variables) is a \n",
    "multiple linear regression.\n",
    "\n",
    "while simple linear regression involves one dependent variable and one independent variable with a linear relationship, multiple \n",
    "linear regression incorporates two or more independent variables to predict the dependent variable in a more complex, multidimensional\n",
    "context.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad1af13-b849-4e6c-8ce8-99a0218647d6",
   "metadata": {},
   "source": [
    "# Ans : 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8063e0f1-d3d5-4923-8510-76afe3fe66a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Linear regression relies on several assumptions for accurate predictions:\n",
    "\n",
    "1. Linearity: The relationship between the variables is assumed to be linear.\n",
    "2. Independence: Residuals (the differences between predicted and actual values) should be independent.\n",
    "3. Homoscedasticity: Residuals should exhibit constant variance across all levels of the independent variable.\n",
    "4. Normality of Residuals: Residuals should follow a normal distribution.\n",
    "5. No Perfect Multicollinearity: Independent variables should not have a perfect linear relationship with each other.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde93e48-b3ac-430c-be92-a20e18e14b55",
   "metadata": {},
   "source": [
    "# Ans : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc52229-d2f2-446f-a4ed-84697dd4cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In a linear regression model, the slope (coefficients) and intercept have specific interpretations:\n",
    "\n",
    "1. Intercept (b₀): It represents the predicted value of the dependent variable when all independent variables are zero.\n",
    "        In many cases, this value may not have a meaningful interpretation, especially if zero is not a realistic or meaningful \n",
    "        value for your data.\n",
    "\n",
    "2. Slope (b₁, b₂, ...): These coefficients represent the change in the dependent variable for a one-unit change in the corresponding\n",
    "        independent variable, holding other variables constant.\n",
    "\n",
    "Let's consider a real-world example: predicting house prices based on the number of bedrooms and the neighborhood's average income.\n",
    "\n",
    "- Intercept (b₀): It represents the predicted house price when the number of bedrooms is zero and the neighborhood's average \n",
    "        income is zero. This might not be meaningful in this context.\n",
    "  \n",
    "- Slope for Bedrooms (b₁): It indicates how much the predicted house price changes for a one-bedroom increase, assuming the \n",
    "        neighborhood's average income remains constant.\n",
    "\n",
    "- Slope for Income (b₂): It indicates how much the predicted house price changes for a one-unit increase in average neighborhood\n",
    "        income, assuming the number of bedrooms remains constant.\n",
    "\n",
    "For example, if b₁ is $50,000, it means that, on average, adding one bedroom to a house increases its predicted price by\n",
    "$50,000, assuming other factors remain constant.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3885ab12-9199-4267-a8c7-9227f7566187",
   "metadata": {},
   "source": [
    "# Ans : 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008307f2-ab63-45ce-b8a1-f4d7312816ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gradient descent is an optimization algorithm used in machine learning to minimize the error or cost function of a model. \n",
    "The goal is to find the optimal set of parameters that result in the best model performance. It iteratively adjusts the model\n",
    "parameters in the direction that reduces the cost function.\n",
    "\n",
    "The process involves calculating the gradient of the cost function with respect to each parameter, indicating the direction \n",
    "of steepest ascent. Then, the parameters are updated in the opposite direction of the gradient, scaled by a learning rate.\n",
    "This process is repeated until convergence or a predefined number of iterations.\n",
    "\n",
    "The learning rate determines the step size in the parameter space, and careful selection is crucial to avoid overshooting or\n",
    "slow convergence. Gradient descent is widely used in training machine learning models, such as linear regression or neural \n",
    "networks, to find the optimal weights that minimize the difference between predicted and actual values, improving the overall \n",
    "model performance.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a058a7-fd4d-4b94-8230-70be20101be2",
   "metadata": {},
   "source": [
    "# Ans : 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9e2b5d-a51e-4fc3-afa9-3e7edf681a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Multiple linear regression is an extension of simple linear regression that considers more than one independent variable to \n",
    "predict the dependent variable. The model is represented as:\n",
    "\n",
    "[ Y = b_0 + b_1X_1 + b_2X_2 + ldots + b_nX_n + varepsilon ]\n",
    "\n",
    "where:\n",
    "- ( Y ) is the dependent variable.\n",
    "- ( b_0 ) is the intercept.\n",
    "- ( b_1, b_2, ldots, b_n ) are the coefficients for the independent variables ( X_1, X_2, ldots, X_n ).\n",
    "- ( varepsilon ) is the error term.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd217b-cd09-4825-bee8-f1c005dd494f",
   "metadata": {},
   "source": [
    "# Ans : 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662266e4-aba0-4736-89bc-2a855577033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Multicollinearity in multiple linear regression occurs when independent variables in the model are highly correlated with \n",
    "each other. This can lead to challenges in estimating the individual effects of each variable on the dependent variable and\n",
    "can result in unstable coefficient estimates.\n",
    "\n",
    "Detection methods include examining correlation matrices or variance inflation factors (VIF). A high correlation between \n",
    "independent variables or VIF values greater than 10 suggests multicollinearity.\n",
    "\n",
    "To address multicollinearity:\n",
    "1. Feature Selection: Remove redundant or highly correlated variables.\n",
    "2. Data Collection: Collect more data to reduce the impact of correlated variables.\n",
    "3. Principal Component Analysis (PCA): Transform correlated variables into a smaller set of uncorrelated variables.\n",
    "4. Ridge Regression or LASSO: These regularization techniques penalize large coefficients, mitigating multicollinearity effects.\n",
    "\n",
    "Addressing multicollinearity is crucial for obtaining stable and reliable results in multiple linear regression analyses.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19104dd9-6d21-4490-bcf4-911e772a3627",
   "metadata": {},
   "source": [
    "# Ans : 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85687ee7-05cf-4efa-8116-433fc37305d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Polynomial regression is an extension of linear regression that allows for modeling nonlinear relationships between the independent and dependent variables. While linear regression assumes a linear relationship, polynomial regression involves fitting a polynomial equation of a higher degree to the data. The polynomial regression model is expressed as:\n",
    "\n",
    "[ Y = b_0 + b_1X + b_2X^2 + ldots + b_nX^n + varepsilon ]\n",
    "\n",
    "where:\n",
    "- ( Y ) is the dependent variable.\n",
    "- ( b_0, b_1, ldots, b_n ) are the coefficients.\n",
    "- ( X ) is the independent variable.\n",
    "- ( X^2, X^3, ldots, X^n ) represent higher-order terms.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b84f601-c172-40dc-897b-1112c52a990f",
   "metadata": {},
   "source": [
    "# Ans : 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bbb5da-bd9f-4db2-a0e8-ee535bbf8ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Advantages of Polynomial Regression:\n",
    "1. Flexibility:** Polynomial regression can model more complex relationships, capturing nonlinear patterns in the data.\n",
    "2. **ccuracy:** It can provide a better fit for datasets with curved or nonlinear trends.\n",
    "3. **Expressiveness:** Enables representation of various shapes, allowing for a more nuanced understanding of the data.\n",
    "\n",
    "**Disadvantages of Polynomial Regression:**\n",
    "1. **Overfitting:** Higher-degree polynomials may fit noise in the data, leading to poor generalization to new data.\n",
    "2. **Interpretability:** Coefficients in polynomial regression may be harder to interpret compared to linear regression.\n",
    "3. **Computational Complexity:** Higher-degree polynomials involve more computations, making the model more computationally expensive.\n",
    "\n",
    "**Use Cases for Polynomial Regression:**\n",
    "1. **Nonlinear Patterns:** When the relationship between variables is clearly nonlinear.\n",
    "2. **Curved Trends:** When the data exhibits curves or bends that a linear model cannot capture adequately.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
